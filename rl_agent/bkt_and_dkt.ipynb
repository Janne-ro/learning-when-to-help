{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b25eef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bkt_and_dkt.py\n",
    "# Requires: numpy, torch\n",
    "# pip install numpy torch\n",
    "\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a74a4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# BKT implementation\n",
    "# ---------------------\n",
    "class BKTModel:\n",
    "    \"\"\"\n",
    "    Classic single-skill BKT:\n",
    "      params: p_init, p_trans, slip, guess\n",
    "      - absorbing learned state (if L_t=1 then L_{t+1}=1)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, p_init=0.1, p_trans=0.1, slip=0.1, guess=0.2):\n",
    "        #set parameters\n",
    "        self.p_init = float(p_init)\n",
    "        self.p_trans = float(p_trans)\n",
    "        self.slip = float(slip)\n",
    "        self.guess = float(guess)\n",
    "\n",
    "    def simulate_student(self, length: int, seed=None) -> List[int]:\n",
    "        \"\"\"Simulate a single student sequence of correctness (0/1).\"\"\"\n",
    "        #initate initial skills\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        L = 1 if random.random() < self.p_init else 0\n",
    "        obs = []\n",
    "        for t in range(length):\n",
    "            # observation for that the student answers correctly on current question\n",
    "            if L == 1:\n",
    "                prob_correct = 1.0 - self.slip\n",
    "            else:\n",
    "                prob_correct = self.guess\n",
    "            c = 1 if random.random() < prob_correct else 0\n",
    "            obs.append(c)\n",
    "            # transition: if not learned, might learn \n",
    "            if L == 0 and random.random() < self.p_trans:\n",
    "                L = 1\n",
    "            # if L==1, stay learned (absorbing)\n",
    "        return obs\n",
    "\n",
    "    def simulate_dataset(self, n_students: int, seq_len: int, rng_seed=None) -> List[List[int]]:\n",
    "        if rng_seed is not None:\n",
    "            random.seed(rng_seed)\n",
    "        return [self.simulate_student(seq_len) for _ in range(n_students)]\n",
    "\n",
    "    # --------------------------\n",
    "    # Forward-backward for HMM\n",
    "    # --------------------------\n",
    "    def _obs_prob(self, c, L):\n",
    "        \"\"\"P(C=c | L)\"\"\"\n",
    "        if L == 1:\n",
    "            return (1.0 - self.slip) if c == 1 else self.slip\n",
    "        else:\n",
    "            return self.guess if c == 1 else (1.0 - self.guess)\n",
    "\n",
    "    def forward_backward(self, seq: List[int]):\n",
    "        \"\"\"Compute forward and backward messages and posteriors for one sequence.\n",
    "           Returns alpha, beta, gamma (posterior of L_t=1), xi (expected transitions 0->1 at t).\n",
    "        \"\"\"\n",
    "        T = len(seq)\n",
    "        # states 0 and 1\n",
    "        # forward alpha_t(s) = P(C_1..C_t, L_t = s)\n",
    "        alpha = np.zeros((T, 2))\n",
    "        # init\n",
    "        alpha[0, 1] = self.p_init * self._obs_prob(seq[0], 1)\n",
    "        alpha[0, 0] = (1 - self.p_init) * self._obs_prob(seq[0], 0)\n",
    "        # scale to avoid underflow\n",
    "        for t in range(1, T):\n",
    "            c = seq[t]\n",
    "            # from previous states:\n",
    "            # if previous was 1, next is 1 (absorbing)\n",
    "            # P(L_t=1) = alpha[t-1,1]*1 + alpha[t-1,0]*p_trans\n",
    "            alpha[t, 1] = (alpha[t-1,1] * 1.0 + alpha[t-1,0] * self.p_trans) * self._obs_prob(c, 1)\n",
    "            # P(L_t=0) = only possible from previous 0 and no learn\n",
    "            alpha[t, 0] = (alpha[t-1,0] * (1.0 - self.p_trans)) * self._obs_prob(c, 0)\n",
    "            # normalization maybe small; keep absolute values for backward correctness\n",
    "        # backward\n",
    "        beta = np.zeros((T, 2))\n",
    "        beta[T-1, :] = 1.0\n",
    "        for t in range(T-2, -1, -1):\n",
    "            c_next = seq[t+1]\n",
    "            # compute contributions to beta[t,s] = sum_{s'} P(L_{t+1}=s'|L_t=s) * P(C_{t+1}|s') * beta[t+1,s']\n",
    "            # for s=1:\n",
    "            #   from s=1 -> s'=1 with prob 1.0\n",
    "            beta[t, 1] = 1.0 * self._obs_prob(c_next, 1) * beta[t+1, 1]\n",
    "            # for s=0:\n",
    "            #   s'=1 with p_trans\n",
    "            #   s'=0 with (1-p_trans)\n",
    "            beta[t, 0] = (self.p_trans * self._obs_prob(c_next, 1) * beta[t+1, 1] +\n",
    "                          (1.0 - self.p_trans) * self._obs_prob(c_next, 0) * beta[t+1, 0])\n",
    "        # gamma (posterior P(L_t = 1 | seq))\n",
    "        gamma = np.zeros(T)\n",
    "        for t in range(T):\n",
    "            numer1 = alpha[t,1] * beta[t,1]\n",
    "            numer0 = alpha[t,0] * beta[t,0]\n",
    "            denom = numer0 + numer1\n",
    "            if denom == 0:\n",
    "                gamma[t] = 0.0\n",
    "            else:\n",
    "                gamma[t] = numer1 / denom\n",
    "        # xi: expected # transitions from 0->1 at time t (i.e., from L_t=0 to L_{t+1}=1)\n",
    "        xi = np.zeros(T-1)\n",
    "        for t in range(T-1):\n",
    "            c_next = seq[t+1]\n",
    "            # joint prob proportional to alpha[t,0] * P(L_{t+1}=1|L_t=0)=p_trans * P(C_{t+1}|1) * beta[t+1,1]\n",
    "            numer = alpha[t,0] * self.p_trans * self._obs_prob(c_next, 1) * beta[t+1,1]\n",
    "            denom = (alpha[t,0] * self._obs_prob(seq[t],0) * beta[t,0] if False else\n",
    "                     (alpha[t,0]*beta[t,0] + alpha[t,1]*beta[t,1]))  # not used; compute normalization below\n",
    "            # instead compute sum over s,s' of alpha[t,s]*P(s->s')*P(obs_{t+1}|s')*beta[t+1,s']\n",
    "            denom_sum = (\n",
    "                alpha[t,0] * ( (1.0-self.p_trans) * self._obs_prob(c_next, 0) * beta[t+1,0]\n",
    "                              + self.p_trans * self._obs_prob(c_next, 1) * beta[t+1,1] )\n",
    "                + alpha[t,1] * (1.0 * self._obs_prob(c_next, 1) * beta[t+1,1])\n",
    "            )\n",
    "            xi[t] = (numer / denom_sum) if denom_sum != 0 else 0.0\n",
    "        return alpha, beta, gamma, xi\n",
    "\n",
    "    def fit(self, sequences: List[List[int]], n_iters=15, verbose=False):\n",
    "        \"\"\"\n",
    "        Fit parameters with EM (Baum-Welch) using multiple sequences (list of lists of 0/1).\n",
    "        \"\"\"\n",
    "        for it in range(n_iters):\n",
    "            # expected counts\n",
    "            sum_gamma0 = 0.0   # expected times in state 0\n",
    "            sum_gamma1 = 0.0   # expected times in state 1\n",
    "            sum_init1 = 0.0\n",
    "            sum_trans_0_to_1 = 0.0  # expected transitions 0->1\n",
    "            sum_correct_in_state1 = 0.0\n",
    "            sum_state1 = 0.0\n",
    "            sum_correct_in_state0 = 0.0\n",
    "            sum_state0 = 0.0\n",
    "\n",
    "            for seq in sequences:\n",
    "                if len(seq) == 0:\n",
    "                    continue\n",
    "                alpha, beta, gamma, xi = self.forward_backward(seq)\n",
    "                T = len(seq)\n",
    "                sum_init1 += gamma[0]\n",
    "                sum_gamma1 += gamma.sum()\n",
    "                sum_gamma0 += (T - gamma.sum())\n",
    "                # expected transitions\n",
    "                sum_trans_0_to_1 += xi.sum()\n",
    "                # expected observation counts\n",
    "                # for state1: expected number of times in state1 and response correct\n",
    "                for t, c in enumerate(seq):\n",
    "                    if gamma[t] > 0:\n",
    "                        if c == 1:\n",
    "                            sum_correct_in_state1 += gamma[t]\n",
    "                        sum_state1 += gamma[t]\n",
    "                        sum_state0 += (1.0 - gamma[t])\n",
    "                        if c == 1:\n",
    "                            sum_correct_in_state0 += (1.0 - gamma[t])\n",
    "                    else:\n",
    "                        # gamma[t]==0 -> contribution to state0 only\n",
    "                        sum_state0 += 1.0\n",
    "                        if c == 1:\n",
    "                            sum_correct_in_state0 += 1.0\n",
    "            # M-step updates\n",
    "            new_p_init = (sum_init1 / len(sequences)) if len(sequences) > 0 else self.p_init\n",
    "            new_p_trans = (sum_trans_0_to_1 / max(1e-8, sum_gamma0))  # expected transitions / expected times in 0\n",
    "            new_slip = 1.0 - (sum_correct_in_state1 / max(1e-8, sum_state1))\n",
    "            new_guess = (sum_correct_in_state0 / max(1e-8, sum_state0))\n",
    "            # clip to (tiny, 1-tiny)\n",
    "            eps = 1e-6\n",
    "            self.p_init = float(min(max(new_p_init, eps), 1-eps))\n",
    "            self.p_trans = float(min(max(new_p_trans, eps), 1-eps))\n",
    "            self.slip = float(min(max(new_slip, eps), 1-eps))\n",
    "            self.guess = float(min(max(new_guess, eps), 1-eps))\n",
    "            if verbose:\n",
    "                print(f\"EM iter {it+1}: p_init={self.p_init:.4f}, p_trans={self.p_trans:.4f}, slip={self.slip:.4f}, guess={self.guess:.4f}\")\n",
    "\n",
    "    def predict_mastery_after_sequence(self, seq: List[int]) -> float:\n",
    "        \"\"\"Return P(L_T = 1 | observations seq) â€” posterior mastery after observed seq.\"\"\"\n",
    "        if len(seq) == 0:\n",
    "            return self.p_init\n",
    "        _, _, gamma, _ = self.forward_backward(seq)\n",
    "        return float(gamma[-1])\n",
    "\n",
    "    def predict_next_correct_prob(self, seq: List[int]) -> float:\n",
    "        \"\"\"Predict P(next response correct | observed seq).\"\"\"\n",
    "        p_mastery = self.predict_mastery_after_sequence(seq)\n",
    "        # next-step mastery: if not learned, can learn with p_trans before next observation?\n",
    "        # Classic BKT assumes learning happens after an opportunity; here we predict on next opportunity:\n",
    "        # If the learning occurs *between* steps, you might update. We'll assume prediction before next learning:\n",
    "        p_correct = p_mastery * (1.0 - self.slip) + (1.0 - p_mastery) * self.guess\n",
    "        return p_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2408cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class MultiSkillBKT:\n",
    "    \"\"\"\n",
    "    Multi-skill BKT: each skill has its own BKT model.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_skills: int, p_init=0.1, p_trans=0.1, slip=0.1, guess=0.2):\n",
    "        # Create one BKTModel per skill\n",
    "        self.skills = [BKTModel(p_init, p_trans, slip, guess) for _ in range(n_skills)]\n",
    "        self.n_skills = n_skills\n",
    "\n",
    "    def simulate_student(self, task_skill_map: List[List[int]], seed=None) -> List[int]:\n",
    "        \"\"\"\n",
    "        Simulate a student sequence of responses for tasks.\n",
    "        task_skill_map: list of lists, each sublist contains skill indices required by that task.\n",
    "        \"\"\"\n",
    "        if seed is not None:\n",
    "            random.seed(seed)\n",
    "        # Initialize mastery state for each skill\n",
    "        L = [1 if random.random() < sk.p_init else 0 for sk in self.skills]\n",
    "        obs = []\n",
    "\n",
    "        for skills_required in task_skill_map:\n",
    "            # compute probability correct\n",
    "            prob_correct = 1.0\n",
    "            for s in skills_required:\n",
    "                if L[s] == 1:\n",
    "                    prob_correct *= (1.0 - self.skills[s].slip)\n",
    "                else:\n",
    "                    prob_correct *= self.skills[s].guess\n",
    "            c = 1 if random.random() < prob_correct else 0\n",
    "            obs.append(c)\n",
    "\n",
    "            # update skill mastery (absorbing)\n",
    "            for s in range(self.n_skills):\n",
    "                if L[s] == 0 and random.random() < self.skills[s].p_trans:\n",
    "                    L[s] = 1\n",
    "\n",
    "        return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0429e0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Task mapping: task -> required skills\n",
    "# Task 1: skill 0\n",
    "# Task 2: skill 1\n",
    "# Task 3: skill 0 and 1\n",
    "task_skill_map = [[0], [1], [0, 1], [0], [1], [0, 1], [0], [1], [0, 1]]\n",
    "\n",
    "ms_bkt = MultiSkillBKT(n_skills=2, p_init=0.2, p_trans=0.1, slip=0.1, guess=0.2)\n",
    "student_obs = ms_bkt.simulate_student(task_skill_map, seed=42)\n",
    "print(student_obs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
